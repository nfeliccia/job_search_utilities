Creating a system that aggregates job postings from various company websites is an ambitious project that involves multiple components, including web scraping or API integration, data storage, and possibly a user interface. Here's how you can approach this:

1. Database Selection:
Given your requirements and the potential to scale, a relational database seems appropriate because you have structured data (e.g., company name, unique ID, base URL, parameters). Among the options, Google Cloud SQL (which supports MySQL, PostgreSQL, and SQL Server) is a robust choice as it's fully-managed, ensures scalability, and offers high availability.

2. Database Schema:
Your database schema for companies might look something like this:

Company

ID (Primary key, Unique)
Name
CareersURL (base URL for job queries)
URLParameters

ID (Primary key, Unique)
CompanyID (Foreign key to Company.ID)
ParameterName
DataType
ValidValues (could be a string that you parse as needed, or a link to another table with valid values if the list is extensive)
3. Web Scraping/API Integration:
You'll need to develop a way to interact with each company's job listing. This could be via web scraping or, if the company provides it, an API. This part of the system will need to be robust to handle changes in the websites you're scraping.

4. User Data Storage:
You also need to store user preferences. This would be another table in your database:

UserPreferences
UserID (Primary key, Unique, possibly linked to a user account table if you have user accounts)
JobKeywords
DesiredLocation
OpenToRemote (boolean)
5. Application Backend:
You'll need to write server-side logic to handle requests such as:

User submits job search criteria.
System translates criteria into a query for each company's system.
System retrieves job postings from each company's system.
System aggregates results and returns them to the user.
6. Frontend:
This could be a web app, mobile app, or both, that provides users with an interface to input their job search criteria and view the aggregated job postings.

7. Porting to Google Cloud:
Once you have your initial implementation, you can port to Google Cloud by:

Using Google Cloud SQL for your database.
Possibly deploying your application on Google App Engine or Google Kubernetes Engine, depending on the scale.
Using other Google Cloud services as needed for functionality, scaling, and reliability.
8. Compliance and Permissions:
Ensure you have the right to use the data from company websites, as scraping can be against the terms of service. Also, consider user data protection laws (like GDPR if you're handling EU citizens' data).

Development Steps:
Local Development: Start by developing your application locally, using a local SQL database.
Cloud Migration: Migrate your local SQL database to Google Cloud SQL.
Application Deployment: Deploy your application to Google Cloud (e.g., App Engine, Kubernetes Engine).
Testing and Scaling: Test the cloud deployment, troubleshoot issues, and plan for scaling.
Remember, this is a high-level overview. Each step involves a lot of detailed work, and there are many sub-components (error handling, data validation, security, etc.) that need careful attention. Also, the technology stack you choose (programming language, web framework, etc.) will impact your project's specifics.